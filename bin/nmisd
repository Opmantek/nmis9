#!/usr/bin/perl
#
#  Copyright 1999-2018 Opmantek Limited (www.opmantek.com)
#
#  ALL CODE MODIFICATIONS MUST BE SENT TO CODE@OPMANTEK.COM
#
#  This file is part of Network Management Information System ("NMIS").
#
#  NMIS is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  NMIS is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with NMIS (most likely in a file named LICENSE).
#  If not, see <http://www.gnu.org/licenses/>
#
#  For further information on NMIS or for a license other than GPL please see
#  www.opmantek.com or email contact@opmantek.com
#
#  User group details:
#  http://support.opmantek.com/users/
#
# *****************************************************************************
use strict;
our $VERSION="9.0.8";

if (@ARGV == 1 && $ARGV[0] eq "--version")
{
	print "version=$VERSION\n";
	exit 0;
}

# local modules live in <nmis-base>/lib
use FindBin;
use lib "$FindBin::Bin/../lib";

use feature 'state';
use File::Basename;
use File::Spec;
use Proc::ProcessTable 0.53;
use Try::Tiny;
use Mojo::File;
use Data::Dumper;
use Time::HiRes;
use Statistics::Lite;
use List::Util 1.33;
use Clone;
use Net::IP;

use NMISNG;
use NMISNG::Util;
use NMISNG::Sys;
use Compat::NMIS;

# for worker_loop
use constant { EXIT_POLITE => 0, EXIT_TTL => 1, EXIT_IMMEDIATELY => 2, EXIT_ERROR => 3, EXIT_IDLE => 4 };

my $me = basename($0);
my $usage = "Usage: $me [option=value...] [act=command]

 act=version: print version of this daemon and exit
 act=stop: shut down running daemon and workers
 act=abort: terminate all workers and kill running daemon

if no act argument is present: daemon starts

option foreground=1: stay in the foreground, don't daemonize
option max_workers=N: overrides the configuration
option debug=0/1: print extra debug information

option confdir=path: path to configuration files\n\n";

die $usage if (@ARGV == 1 && $ARGV[0] =~ /^-(h|\?|-help)$/);
my $cmdline = NMISNG::Util::get_args_multi(@ARGV);
die $usage if ($cmdline->{act} && $cmdline->{act} !~ /^(stop|abort|version)$/);

if ( $cmdline->{act} eq "version")
{
	print "$me version=$VERSION\n";
	exit 0;
}

# get the config from a potentially custom directory
my $config  = NMISNG::Util::loadConfTable(dir => $cmdline->{confdir});
die "no config available!\n" if (ref($config) ne "HASH" or !keys %$config);

# use debug, or info arg, or configured log_level
my $logfile = "$config->{'<nmis_logs>'}/nmis.log";
# create the logfile so that permissions can be adjusted
Mojo::File->new($logfile)->spurt("") if (!-f $logfile);
NMISNG::Util::setFileProtDiag(file =>$logfile) if (-f $logfile);
my $logger = NMISNG::Log->new( level => NMISNG::Log::parse_debug_level(
																 debug => $cmdline->{debug}) // $config->{log_level},
															 path  =>  (defined($cmdline->{debug})? undef : $logfile));
# this opens a database connection - which we must not share with any children!
my $nmisng = Compat::NMIS::new_nmisng(config => $config, log => $logger);
&NMISNG::rrdfunc::require_RRDs;

# Temporary directory
my $tmpdir = $config->{"<nmis_tmp>"} || $config->{"<nmis_var>"} . "/tmp" || "/tmp";
$ENV{NMISTMPDIR} = "$tmpdir";

# make sure that no more than one daemon runs
my $varsysdir = $config->{'<nmis_var>'} . "/nmis_system";
if ( !-d $varsysdir )
{
	NMISNG::Util::createDir($varsysdir);
	NMISNG::Util::setFileProtDiag(file =>$varsysdir);
}
my $pidfile = "$varsysdir/nmisd.pid";
my $otherdaemon = Mojo::File->new($pidfile)->slurp() if (-f $pidfile);
chomp($otherdaemon);

# for act=stop/abort we need to load the previous state to examine any zoo of worker procs
my $statefile = "$varsysdir/nmisd_state.json";
my $scheduler_state = ( -f $statefile? NMISNG::Util::readFiletoHash(file => $statefile): {} );
$scheduler_state = {} if (!ref($scheduler_state)); # i.e. if readFiletoHash reported an error
$scheduler_state->{last_process_check} = 0; # DO run up worker children on startup
my @otherprocs = (ref($scheduler_state->{zoo}) eq "HASH"?
									keys %{$scheduler_state->{zoo}} : ());

if ( int($otherdaemon)				# it's a number
		 and $otherdaemon != $$	# and it's not me (should be impossible)
		 and kill(0, $otherdaemon)) # and it's really alive
{
	die "Another instance of $me is running (pid $otherdaemon)!\n"
			if  ($cmdline->{act} !~ /^(stop|abort)$/);

	# int: please shut down when you're ready. term: shut down now
	# note: signalling that scheduling daemon will take care of its workers as well
	my $whichsignal = $cmdline->{act} eq "abort"? "TERM":"INT";
	kill($whichsignal, $otherdaemon);		# polite
	sleep(1);
	kill('KILL', $otherdaemon);					# then  firm
	unlink($pidfile);
	$logger->info("Killed process $otherdaemon as instructed.");
	exit (0);
}
# if stop/abort, get rid of these; if starting up
# they'll get 'adopted' by this scheduler daemon
elsif (@otherprocs && $cmdline->{act} =~ /^(stop|abort)$/)
{
	my $whichsignal = $cmdline->{act} eq "abort"? "TERM":"INT";
	kill($whichsignal, @otherprocs);
	sleep(1);
	kill("KILL", @otherprocs);
	$logger->info("Killed worker processes ".join(" ",@otherprocs). " as instructed");
	exit(0);
}

# allowed to become a daemon? foreground mode is mostly just for debugging
if (!NMISNG::Util::getbool($cmdline->{foreground}))
{
	if (!defined(my $pid = fork))
	{
		die "cannot fork: $!\n";
	}
	elsif ($pid)
	{
		# parent: exits
		exit (0);
	}

	# child continues with the actual work
	srand();
	$nmisng->log->logprefix("$me\[$$\] ");
	$nmisng->log->info("$me daemon was started (pid $$).");


	POSIX::setsid() or die "Can't start new session: $!";
	chdir('/') or die "Can't chdir to /: $!";

	# Reopen stdout, stdin with /dev/null; stderr to the main logfile
	# if we dont reopen, the calling terminal will wait, and nmis.pl daemon control will hang
	open(STDIN, "<", "/dev/null") or die "cannot reopen stdin: $!\n";
	# leave those open if debug mode enabled
	if (!defined($cmdline->{debug}))
	{
		open(STDOUT, ">", "/dev/null") or die "cannot reopen stdout: $!\n";
		open(STDERR, ">>", $logfile) or die "cannot open stdout to $logfile: $!\n";
	}
	$0 = "nmisd.scheduler";
}
Mojo::File->new($pidfile)->spurt("$$\n");
NMISNG::Util::setFileProtDiag(file => $pidfile); # or the selftest complains

my $zoo = ($scheduler_state->{zoo} //= {}); # process state structure for manage_processes
my $exit_marker;							# signal handler scribbles here

# signal handlers: INT and TERM for shutdown, HUP for log reopening
# and these need forwarding to the child processes
$SIG{"HUP"} = sub {
	my ($sig) = @_;
	$nmisng->log->debug("received $sig, reopening logfile");
	$nmisng->log->reopen;
	$nmisng->log->debug("logfile was reopened");
	# and forward...
	kill($sig, keys %$zoo) if (keys %$zoo);
};
$SIG{"TERM"} = $SIG{"INT"} = sub {
	my ($signame) = @_;
	$nmisng->log->info("received $signame, shutting down");
	$exit_marker = $signame;
	kill($signame, keys %$zoo) if (keys %$zoo);

	# int means when ready, term means now
	exit(0) if ($signame eq "TERM");
};

# signal handler for USR1/USR2 is just for this process
$SIG{"USR1"} = $SIG{"USR2"} = sub { &verbosity_signal_handler(shift, $nmisng->log); };

if ( !-d $tmpdir )
{
	NMISNG::Util::createDir($tmpdir);
}
NMISNG::Util::verifyNMISEncryption();


while (!$exit_marker)
{
	my $ready_jobs;								# marker set if there are worker-jobs ready to start

	# nmis locked? don't do anything at all
	if (-f "$config->{'<nmis_conf>'}/NMIS_IS_LOCKED")
	{
		$nmisng->log->info("nmis is locked, sleeping 60 seconds");
		sleep(60);
		next;
	}

	# look for type=schedule jobs - that's work for the supervisory daemon itself
	#	fixme9: not implemented yet

	# overdue stuck jobs? abort, terminate the worker (if still around), and report
	# first look for jobs that might be stuck,
	# ie. anything older than the lowest maximum acceptable age
	my $bestbefore = Statistics::Lite::min(grep(defined($_) && $_ > 0,
																							map { $config->{$_} } (grep(/^abort_\S+_after$/, keys %$config))));
	if ($bestbefore && $bestbefore > 0)
	{
		my $now = Time::HiRes::time;
		my $maybestuck = $nmisng->get_queue_model(
			in_progress => { '$gt' => 0, '$lt' => ($now-$bestbefore) });
		if (my $error = $maybestuck->error)
		{
			$nmisng->log->error("Failed to query job queue: $error");
		}
		else
		{
			for my $onejob (@{$maybestuck->data})
			{
				# then ensure the job is past the actual job type's use by date
				my $realmaxage = $config->{"abort_$onejob->{type}_after"};
				if (defined($realmaxage) && $realmaxage > 0 && $onejob->{in_progress} < ($now - $realmaxage) )
				{
					$nmisng->log->error("Job $onejob->{_id}, type $onejob->{type}"
														 . ($onejob->{type} =~ /^(collect|update|services)$/?
																" for node $onejob->{args}->{uuid}":"")
														 . " is too old, in progress for "
														 .int($now - $onejob->{in_progress})."s, maximum allowed ${realmaxage}s");
					# remove the job, terminate the worker (if one is left)
					my $workerpid = $onejob->{status}->{pid};

					$nmisng->log->warn("Aborting job $onejob->{_id} due to age");
					my $error = $nmisng->remove_queue(id => $onejob->{_id});
					$nmisng->log->error("Failed to remove job: $error") if ($error); # not terminal though

					if (exists($zoo->{$workerpid}))
					{
						$nmisng->log->warn("Terminating worker process $workerpid that owned $onejob->{_id} for too long");
						kill("TERM",$workerpid);
					}

					# find and update any inprogress opstatus record in question - context.queue_id links back to the queue
					my $oldopstatus = $nmisng->get_opstatus_model("context.queue_id" => $onejob->{_id},
																												status => "inprogress"  );
					if (my $error = $oldopstatus->error)
					{
						$nmisng->log->error("Failed to query opstatus: $error");
					}
					my ($oldstatusid,$oldcontext,$oldact);
					if ($oldopstatus->count == 1)
					{
						($oldstatusid,$oldcontext,$oldact)
								= @{$oldopstatus->data->[0]}{"_id","context","activity"};
					}
					$nmisng->save_opstatus(id => $oldstatusid,
																 activity => $oldact // $onejob->{type},
																 status => "error",
																 type => "aborted",
																 details => ("Aborted job: too old, in progress for "
																						 .int($now - $onejob->{in_progress})."s, maximum allowed ${realmaxage}s"),
																 stats => undef,
																 context => $oldcontext,
							);
				}
			}
		}
	}

	# handle policy-controlled operations (services, node updates and node polling),
	# attention: these need tagging so that after-op plugins are schedulable!
	for my $operation (qw(update collect services))
	{
		$nmisng->log->debug2("Looking for nodes due for $operation operation");
		# only consider our own nodes
		my $duenodes = $nmisng->find_due_nodes(type => $operation,
																					 filters => [ { cluster_id => $config->{cluster_id } } ]);
		if ($duenodes->{error})
		{
			$nmisng->log->error("Failed to check due nodes for $operation: $duenodes->{error}");
			next;
		}
		my $nrtodo = scalar(keys %{$duenodes->{nodes}});
		$nmisng->log->info("Found $nrtodo nodes due for $operation operation");
		next if (!$nrtodo);

		my $tag = int(rand(1<<31));
		$nmisng->log->info("Jobs for this $operation operation share tag \"$tag\"");
		for my $nodeuuid (keys %{$duenodes->{nodes}})
		{
			# let's hand the work to somebody!
			# but only if nothing interferes, ie. no in-progress jobs nor overdue ones
			if (ref($duenodes->{in_progress}) eq "HASH"
					&& exists($duenodes->{in_progress}->{$nodeuuid}))
			{
				$nmisng->log->warn("Not queueing $operation for node $nodeuuid ($duenodes->{nodes}->{$nodeuuid}->{name}), job already in progress!");
				next;
			}
			if (ref($duenodes->{overdue}) eq "HASH"
					&& exists($duenodes->{overdue}->{$nodeuuid}))
			{
				$nmisng->log->warn("Not queueing $operation for node $nodeuuid ($duenodes->{nodes}->{$nodeuuid}->{name}), an overdue job is queued already!");
				next;
			}

			# fixme: should the schedules be spread out a bit over time? how much and under what circumstances?
			# maybe use number of workers a/v versus number of due jobs in queue?
			my $jobdueat = Time::HiRes::time;

			my %jobargs = (uuid => $nodeuuid);
			if( $operation eq "collect")
			{
				$jobargs{wantwmi} = $duenodes->{flavours}->{$nodeuuid}->{wmi};
				$jobargs{wantsnmp} = $duenodes->{flavours}->{$nodeuuid}->{snmp};
			}
			elsif ($operation eq "services")
			{
				$jobargs{services} = $duenodes->{services}->{$nodeuuid};
			}

			my ($error, $qid) = $nmisng->update_queue( jobdata =>
																								 {
																									 type => $operation,
																									 time => $jobdueat,
																									 priority => # newly added nodes should get updated FIRST
																											 ($operation eq "update" && $duenodes->{newnodes}->{$nodeuuid}?
																												1 : $config->{"priority_$operation"} // 0.5),
																									 in_progress => 0,
																									 args => \%jobargs,
																									 tag => $tag });
			if ($error)
			{
				$nmisng->log->error("failed to queue job $operation on $nodeuuid: $error!");
				next;
			}
			$nmisng->log->debug2("Queued job $operation for $nodeuuid ($duenodes->{nodes}->{$nodeuuid}->{name})");
			++$ready_jobs;

			# memorise the tagged jobs that are in flight,
			# so that we can trigger after-collect/after-update plugins (not required for services)
			if ($operation eq "collect" or $operation eq "update")
			{
				$scheduler_state->{in_flight}->{$operation}->{$tag}->{$nodeuuid}
				=  [ (ref($qid) =~ /^(BSON|MongoDB)::OID$/? $qid->TO_JSON: $qid),
						 $jobdueat ];
			}
		}
	}

	# then look at completed related jobs, ie. same op and same tag
	# and schedule post-xyz operations (e.g. plugins, stats collection like total time)
	#
	# in_flight carries operation, tag, uuid and queue id plus desired start time
	# check op+tag, any jobs left in queue? wait longer. none left? schedule plugin
	# fixme: should we add logic to give up on plugins for stuck/overdue jobs? or leave that to nmis-cli act=abort
	for my $icanhasplugin (qw(update collect))
	{
		$nmisng->log->debug2("Checking (un)finished $icanhasplugin jobs");
		next if (ref($scheduler_state->{in_flight}->{$icanhasplugin}) ne "HASH"
						 or !keys %{$scheduler_state->{in_flight}->{$icanhasplugin}});

		for my $tag (keys %{$scheduler_state->{in_flight}->{$icanhasplugin}})
		{
			my $thisflock = $scheduler_state->{in_flight}->{$icanhasplugin}->{$tag};
			if (ref($thisflock) ne "HASH" or !keys %$thisflock)
			{
				delete $scheduler_state->{in_flight}->{$icanhasplugin}->{$tag};
				next;
			}

			# any instances for this flock of jobs left in the queue?
			my $arewethereyet = $nmisng->get_queue_model(type => $icanhasplugin,
																									 tag => $tag);
			if (my $error = $arewethereyet->error)
			{
				$nmisng->log->error("failed to query job queue: $error");
				next;
			}
			if (my $notyetdone = $arewethereyet->count)
			{
				$nmisng->log->debug3("$notyetdone incomplete $icanhasplugin jobs with tag \"$tag\" present");
				next;
			}
			# looks like it's time for a post-op plugin, so let's schedule one
			# IFF there are any plugins that provide after_xyz_plugin functions...
			if (grep( $_->can("after_${icanhasplugin}_plugin"), $nmisng->plugins))
			{
				# Check for existance of the same job
				my $jobs = $nmisng->get_queue_model(type => 'plugins' , # a job for a worker
									'args.phase' => $icanhasplugin, # post-collect or post-update plugins
									'args.uuid' => [ keys %$thisflock ] # these were the nodes in the op
				);

				if ( scalar(@{$jobs->{_data}}) > 0 ) {
					$nmisng->log->debug("Already exist " . scalar(@{$jobs->{_data}}) . " jobs for plugins and uuid " . Dumper([ keys %$thisflock ]));
					$nmisng->log->info("Already exist " . scalar(@{$jobs->{_data}}) . " jobs for plugins and uuid. Skipping job ");
				} else {
					$nmisng->log->info("Scheduling post-$icanhasplugin plugin execution for tag $tag and "
																 .(scalar keys %$thisflock)." nodes");
		
						my ($error, $qid) = $nmisng->update_queue( jobdata =>
																											 {
																												 type => "plugins",
																												 time => Time::HiRes::time,
																												 priority =>
																														 $config->{"priority_plugins"} // 0.5,
																												 in_progress => 0,
																												 args => {
																													 phase => $icanhasplugin, # post-collect or post-update plugins
																													 uuid => [ keys %$thisflock ], # these were the nodes in the op
																												 },
																												 tag => $tag });
						if ($error)
						{
							$nmisng->log->error("failed to queue post-$icanhasplugin plugins for tag $tag: $error!");
							next;
						}
				}
		
				
			}

			# we're done with this flock
			delete $scheduler_state->{in_flight}->{$icanhasplugin}->{$tag};
			++$ready_jobs;
		}
	}


	# fixme9! how about reports? schedulable or not?

	# consult the ops frequencies config for non-node-specific schedulable operations
	# escalate and metrics were post-collect in the past, but unpleasant to schedule that way
	my @schedulables = (qw(escalations metrics configbackup purge dbcleanup selftest permission_test));
	# thresholds are done for each node unless threshold_poll_node is set to false
	push @schedulables, "thresholds"
			if (NMISNG::Util::getbool($config->{global_threshold})
					and !NMISNG::Util::getbool($config->{threshold_poll_node}));
	for my $automagic (@schedulables)
	{
		my $period = $config->{"schedule_$automagic"};
		if (!exists($config->{"schedule_$automagic"}))
		{
			$nmisng->log->warn("Configuration is lacking schedule_$automagic, falling back to 5min");
			$period = 300;
		}
		next if (!defined($period) or $period <= 0);
		my $jobtype = $automagic;

		# don't schedule another if an instance is already in progress
		my $running = $nmisng->get_queue_model(type => $jobtype,
																					 in_progress => { '$ne' => 0 });
		$nmisng->log->error("failed to query job queue: ".$running->error)
				if ($running->error);
		# also don't schedule anything if overdue instances exist,
		# or if a job is 'almost due' (arbitrary choice: now plus one minute, or half the job's period,
		# whichever is shorter)
		my $overdue_or_almost = Time::HiRes::time + Statistics::Lite::min($period/2, 60);
		my $overdue = $nmisng->get_queue_model(type => $jobtype,
																					 time => { '$lt' => $overdue_or_almost },
																					 in_progress => 0 );
		$nmisng->log->error("failed to query job queue: ".$overdue->error)
				if ($overdue->error);

		if (!$running->error and !$overdue->error
				and ($running->count or $overdue->count))
		{
			$nmisng->log->warn("Not scheduling $automagic operation: "
												 .$running->count." instance(s) currently running, "
												 . $overdue->count ." overdue (or nearly so)");
			++$ready_jobs if ($overdue->count); # mostly for picking up in foreground/debug mode
			next;
		}

		# check when this operation was last tried/run/performed, based on opstatus entries except inprogress
		my $mostrecent = $nmisng->get_opstatus_model(activity => $jobtype,
																								 # failure is always an option...actually ok here
																								 status => { '$ne' => "inprogress" },
																								 sort => { 'time' => -1 },
																								 limit => 1);
		my $previous = ($mostrecent->error or !$mostrecent->count)? 0 : $mostrecent->data->[0]->{time};

		# selftest: also depends on the existence of the selftest file
		# which is removed by the gui to enforce a refresh soonest
		$previous = 0 if (($automagic eq "selftest" or $automagic eq "permission_test")
											and !-f "$varsysdir/selftest.json");

		if (Time::HiRes::time - $previous > $period)
		{
			$nmisng->log->debug("Scheduling job $automagic, last scheduled/performed at "
													. ($previous? sprintf("%s (%ds ago)",
																								$previous,
																								time - $previous): "n/a"));

			# due now, let's hand it to somebody
		  my ($error,$qid) = $nmisng->update_queue(jobdata => {
				type => $jobtype,
				time => Time::HiRes::time,
				# some ops are mostly nice-to-have, others are much more critical
				priority => $config->{"priority_$jobtype"} // 0.5,
				in_progress => 0, } );

			if ($error)
			{
				$nmisng->log->error("failed to queue job: $error!");
			}
			++$ready_jobs;
		}
		else
		{
			$nmisng->log->debug3("skipping $automagic, not due: last performed at $previous");
		}
	}

	# anything added for the workers? then signal the idle ones to get the job picked up asap
	if ($ready_jobs)
	{
		my @workers = grep($zoo->{$_}->{processtype} eq "worker", keys %$zoo);
		if (@workers)
		{
			$nmisng->log->debug2("signalling workers about ready-to-pick jobs");
			kill("URG", @workers);
		}
	}

	# in foreground/debugging mode there are no workers to delegate to
	# instead, handle up to 5 ops sequentially and run one fping cycle
	# so foreground 1, singleprocess 0: supervisor stays in FG but workers are created
	# foreground 1, singleprocess 1: only supervisor process and nothing else
	# singleprocess is ignored if foreground is 0.
	if (NMISNG::Util::getbool($cmdline->{foreground})
			&& NMISNG::Util::getbool($cmdline->{singleprocess}))
	{
		# first one fping...
		$nmisng->log->info("Running one fping worker cycle - in foreground mode");
		my $dummymarker;
		my $status = fping_loop(nmisng => $nmisng,
														exit_marker => \$dummymarker,
														max_cycles => 1);
		$nmisng->log->info("Fping cycle returned status $status");

		# ...then up to 5 worker cycles
		for my $seq (1..5)
		{
			$nmisng->log->info("Running one worker cycle - in foreground mode");
			my $dummymarker;
			my $status = worker_loop(nmisng => $nmisng,
															 max_cycles => 1, #
															 exit_marker => \$dummymarker);

			$nmisng->log->info("Single worker cycle returned status $status");
			# save the state more often for foreground/debug mode
			NMISNG::Util::writeHashtoFile(file => $statefile, data => $scheduler_state);

			if( defined($cmdline->{max_jobs}) )
			{
				$cmdline->{max_jobs}--;
				$exit_marker = 1 if($cmdline->{max_jobs} < 1);
			}
			last if ($status == EXIT_IDLE or $exit_marker);
		}
	}
	else	# process management only in non-debug mode
	{
		# every 30s: check the process zoo and herd some cats,
		# also tally the number of processes and execution stats and update the nmis rrd
		# fixme nmis rrd: what can we collect and park there?
		if (!$scheduler_state->{last_process_check}
				or time - $scheduler_state->{last_process_check} >= 30)
		{
			$nmisng->log->debug2("running a worker process check");
			# manage_processes logs by itself - fixme: res not useful at this time
			my $res = manage_processes(nmisng => $nmisng,
																 max_workers => $cmdline->{max_workers},
																 zoo => $zoo);
			$scheduler_state->{last_process_check} = time;
		}
	}

	# collect some queue status information: how many queued, overdue, in-progress
	# and track the last 30 minutes in the scheduler state,
	# for trend detection and warning
	my $now = Time::HiRes::time;
	$scheduler_state->{queue_states} //= [];
	my $queued = $nmisng->get_queue_model(limit => 0, # want no data, just the count
																				count => 1, in_progress => 0);
	if (my $error = $queued->error)
	{
		$nmisng->log->error("Failed to count queued jobs: $error");
	}
	my $overdue = $nmisng->get_queue_model(limit => 0, count => 1,
																				 # allow up to one nmisd_worker_cycle of delay
																				 time => { '$lt' => $now - $config->{nmisd_worker_cycle} },
																				 in_progress => 0);
	if (my $error = $overdue->error)
	{
		$nmisng->log->error("Failed to count overdue jobs: $error");
	}
	my $active = $nmisng->get_queue_model(limit => 0, count => 1, in_progress => { '$ne' => 0 });
	if (my $error = $active->error)
	{
		$nmisng->log->error("Failed to count in-progress jobs: $error");
	}

	my @keepsakes = grep( ($now - $_->[0]) < 1800, @{$scheduler_state->{queue_states}});
	push @keepsakes, [ $now, $queued->query_count,
										 $active->query_count, $overdue->query_count ];
	$scheduler_state->{queue_states} = \@keepsakes;

	# warn if there are more overdue jobs than 2 times the allowed workers,
	$nmisng->log->warn("Performance warning: ".$overdue->query_count." overdue queued jobs!")
			if ($overdue->query_count > 2 * $config->{nmisd_max_workers});

	# warn if there are more queued jobs than 4 times the active nodes,
	# or 10 times the allowed workers, whichever is higher.
	# as collect, update, services are schedulable per node just counting workers is not good
	my $lotsaqueued = $queued->query_count;
	my $nrnodes = $nmisng->get_nodes_model(filter => { active => 1},
																				 count => 1, limit => 0)->query_count; # ignores any errors

	$nmisng->log->warn("Performance warning: $lotsaqueued queued jobs!")
			if ($lotsaqueued > Statistics::Lite::max(4 * $nrnodes, 10 * $config->{nmisd_max_workers}));

	# reap any finished processes
	my %goners = &NMISNG::Util::reaper;
	$nmisng->log->debug2("reaper cleared these pids and exitcodes: "
											 . Data::Dumper->new([\%goners])->Terse(1)->Indent(0)->Pair(": ")->Dump)
			if (%goners && $nmisng->log->is_level(2)); # don't bother with the dumper unless debug2 or higher


	# collect job performance statistics every N seconds and write them to the nmis-system rrd
	# try to catch-up while possible
	my $stats_period = 60; 				# must align with common-database/timing/nmis AND model/database/timing/nmis
	my $S = NMISNG::Sys->new(nmisng => $nmisng);
	$S->init;										# need global/non-node/nmis-system mode

	while (!$scheduler_state->{last_stats_time}
				 or $now - $scheduler_state->{last_stats_time} >= $stats_period)
	{
		my $periodstart = $scheduler_state->{last_stats_time} || $now - $stats_period;
		$scheduler_state->{last_stats_time} = $periodstart + $stats_period;
		$nmisng->log->debug2("updating nmis performance stats, $stats_period seconds from $periodstart");

		# various activity names from get_job_stats_model need shortening as DS names are limited to 19 chars
		my %xlat = ( 'permission_test' => 'perms',
								 'configbackup' => 'backup',
								 'post-update plugins' =>  'postupdate',
								 'post-collect plugins' => 'postcollect',
								 'escalations' => 'esc',
								 'dbcleanup' =>  'cleanup', );

		my %statsdata;
		for my $ds (map { ("${_}time", "${_}count") } (qw(collect update services metrics
selftest perms backup purge cleanup esc thresholds metrics postupdate postcollect delete_nodes update_nodes create_nodes set_nodes
)))
		{
			# no occurrences of op X -> count must be zero, not undefined.
			# undefined is ok for cumulative time values, because
			# we want rrdtool to compute max and avgs for times when such data is available
			$statsdata{$ds} = { option => "gauge,0:U", value => $ds =~ /count$/? 0 : 'U' };
		}
		for my $ds (qw(procs queuedjobs activejobs overduejobs))
		{
			$statsdata{$ds} = { option => "gauge,0:U", value => 0 };
		}

		# first get the data from zoo and queue_states
		$statsdata{procs}->{value} = scalar keys %{$scheduler_state->{zoo}};
		my $inperiodcount;
		for my $inperiod (grep($_->[0] >= $periodstart && $_->[0] <= $periodstart + $stats_period,
													 @{$scheduler_state->{queue_states}}))
		{
			++$inperiodcount;
			for ([1,'queuedjobs'],[2,'activejobs'],[3,'overduejobs'])
			{
				my ($source,$target) = @$_;
				$statsdata{$target}->{value} += $inperiod->[$source];
			}
		}
		if ($inperiodcount)
		{
			map { $statsdata{$_}->{value} /= $inperiodcount } (qw(queuedjobs activejobs overduejobs));
		}

		# then get the timing and count data from opstatus
		my $md = $nmisng->get_job_stats_model(filter => { 'time' => { '$gte' => $periodstart,
																																	'$lt' => $periodstart + $stats_period }});
		if (my $error = $md->error)
		{
			$nmisng->log->error("failed to collect nmis performance stats: $error");
		}
		else
		{
			for my $statsentry (@{$md->data})
			{
				my $dsprefix = $xlat{$statsentry->{activity}} // $statsentry->{activity};
				if (!exists $statsdata{"${dsprefix}time"})
				{
					$nmisng->log->error("skipping unrecognised opstatus activity type \"$dsprefix\"!");
					next;
				}

				$statsdata{"${dsprefix}time"}->{value} = $statsentry->{totaltime};
				$statsdata{"${dsprefix}count"}->{value} = $statsentry->{totalcount};
			}
		}

		$nmisng->log->debug3("inserting nmis performance stats for period $periodstart - "
												. ($periodstart+$stats_period).": "
												. Dumper({ map { ($_ => $statsdata{$_}->{value}) } (keys %statsdata) }))
				if ($nmisng->log->detaillevel >= 3);

		if (!$S->create_update_rrd(type => "nmis", data => \%statsdata))
		{
			$nmisng->log->error("failed to update nmis performance stats: ".NMISNG::rrdfunc::getRRDerror() );
		}
	}

	# save the scheduler state, in case the daemon restarts
	# (local state and file is cheaper than trawling opstatus)
	NMISNG::Util::writeHashtoFile(file => $statefile, data => $scheduler_state, conf => $config);

	# trivial SIGURG listener so that the outer supervisor also is wakeable by others
	$SIG{"URG"} = sub { (my $sig) = @_; $nmisng->log->debug2("received $sig"); };
	sleep($config->{nmisd_scheduler_cycle} // 10);
	$SIG{"URG"} = "IGNORE";
}
$nmisng->log->info("$me terminating");
exit(0);

# ensure that the right number of worker and fping processes (and opslad)
# are present
# args: nmisng object, zoo (state of current processes), max_workers (override for config)
# returns: hashref, success/error
sub manage_processes
{
	my (%args) = @_;
	my ($nmisng,$zoo) = @args{"nmisng","zoo"};

	# zoo structure:
	# pid => { processtype (e.g. worker, fping), start(time)}

	# the record of all the cats we're herding
	my $config = $nmisng->config;
	my %result;

	# figure out the actual state,
	# make sure that only the right processes remain in our list
	# then (re)start or stop stuff to get back to the desired state
	my $pt = Proc::ProcessTable->new(enable_ttys => 0);
	my %bypid = map { ($_->pid => $_) } (@{$pt->table});

	for my $minion (keys %$zoo)
	{
		my $minioninfo = $zoo->{$minion};

		# is the process still around and is it the right one?
		# nmis changes $0 === cmndline, so not very useful for id,
		# but the starttime should remain as-is
		if (!exists($bypid{$minion})	# gone
				or $minioninfo->{starttime} != $bypid{$minion}->start # different process
				or !kill(0, $minion))															# not our process
		{
			$nmisng->log->info("Process $minion ($minioninfo->{processtype}) has terminated");
			delete $zoo->{$minion};
		}
	}

	# start exactly one worker dedicated to fping if configured to
	my @fpingworkers = grep($zoo->{$_}->{processtype} eq "fping", keys %$zoo);
	my $wantfpingworker = NMISNG::Util::getbool($config->{nmisd_fping_worker});

	if (@fpingworkers && !$wantfpingworker)
	{
		$nmisng->log->info("fping worker(s) present but configured for none");
		for my $moriturus (@fpingworkers)
		{
			$nmisng->log->debug("instructing fping worker $moriturus to terminate");
			kill("TERM", $moriturus);
			delete $zoo->{$moriturus};
		}
	}
	elsif (!@fpingworkers && $wantfpingworker)
	{
		$nmisng->log->info("no fping worker active, starting one");

		my $newpid = fork;
		if (!defined $newpid)
		{
			$nmisng->log->error("failed to fork: $!");
			$result{error} = "failed to fork: $!";
		}
		elsif (!$newpid)					# the child, fping process
		{
			# the fping worker process
			srand();
			# make sure this gets a NEW nmisng object, with NEW database handles!
			$nmisng = Compat::NMIS::new_nmisng(nocache => 1,
																				 debug => $cmdline->{debug});
			if (!defined $cmdline->{debug})
			{
				# fping worker logs to its own logfile
				my $fpinglogfile = $config->{'fping_log'};
				# create it so that the permissions can be adjusted
				Mojo::File->new($fpinglogfile)->spurt("") if (!-f $fpinglogfile);
				NMISNG::Util::setFileProtDiag(file => $fpinglogfile);
				$nmisng->log->path($fpinglogfile);

			}
			$nmisng->log->logprefix("fping\[$$\] ");
			$nmisng->log->info("started");
			$0 = "nmisd.fping";

			my $exit_marker;
			# setup signal handlers: HUP and USR1/USR2, INT, TERM
			$SIG{"HUP"} = sub {
				my ($sig) = @_;
				$nmisng->log->debug("received $sig, reopening logfile");
				$nmisng->log->reopen;
				$nmisng->log->debug("logfile was reopened");
			};
			$SIG{"USR1"} = $SIG{"USR2"} = sub { &verbosity_signal_handler(shift, $nmisng->log); };

			$SIG{"TERM"} = $SIG{"INT"} = sub { my ($signame) = @_;
																				 $nmisng->log->info("received $signame, shutting down");
																				 $exit_marker = $signame;
																				 exit(0) if ($signame eq "TERM"); };

			# run fping loop (until signalled to shutdown)
			my $status = fping_loop(nmisng => $nmisng,
															exit_marker => \$exit_marker,
															max_cycles => $config->{nmisd_worker_max_cycles});
			exit( $status != EXIT_ERROR ? 0 : EXIT_ERROR );
		}
		else											# the parent
		{
			$nmisng->log->debug("started fping worker process $newpid");
			# starttime is a tad hard to access and just guessing may be a bad idea
			my ($newinfo) = grep($_->pid == $newpid, @{$pt->table}); # there must be exactly one
			$zoo->{$newpid} = { processtype => "fping",
													starttime => $newinfo->start };
		}
	}

	# workers: how many are we allowed to have?
	my $allowed = $args{max_workers} // $config->{nmisd_max_workers};
	my $current = grep($_->{processtype} eq "worker", values %$zoo);

	my $delta = $allowed - $current;
	if ($delta < 0)
	{
		$nmisng->log->info("too many active workers, allowed $allowed but have $current");
		for my $moriturus ((keys %$zoo)[0..(-$delta)]) # keys is random enough
		{
			$nmisng->log->debug("instructing worker $moriturus to terminate");
			kill("TERM", $moriturus);
			delete $zoo->{$moriturus};
		}
	}
	elsif ($delta > 0)
	{
		$nmisng->log->info("not enough active workers, allowed $allowed but only have $current");
		# start N new ones
		while ($delta--)
		{
			my $newpid = fork;
			if (!defined $newpid)
			{
				$nmisng->log->error("failed to fork: $!");
				$result{error} = "failed to fork: $!";
			}
			elsif (!$newpid)					# the child, worker process
			{
				# worker process:
				srand();
				# make sure this gets a NEW nmisng object, with NEW database handles!
				$nmisng = Compat::NMIS::new_nmisng(nocache => 1, debug => $cmdline->{debug});
				$nmisng->log->logprefix("worker\[$$\] ");
				$nmisng->log->info("started");
				$0 = "nmisd.worker.<idle>";

				my $exit_marker;
				# setup signal handlers: HUP and USR1/USR2, INT, TERM
				$SIG{"HUP"} = sub {
					my ($sig) = @_;
					$nmisng->log->debug("received $sig, reopening logfile");
					$nmisng->log->reopen;
					$nmisng->log->debug("logfile was reopened");
				};
				$SIG{"USR1"} = $SIG{"USR2"} = sub { &verbosity_signal_handler(shift, $nmisng->log); };

				$SIG{"TERM"} = $SIG{"INT"} = sub { my ($signame) = @_;
																					 $nmisng->log->info("received $signame, shutting down");
																					 $exit_marker = $signame;
																					 exit(0) if ($signame eq "TERM"); };

				# run worker loop (until signalled to shutdown)
				my $status = worker_loop(nmisng => $nmisng,
																exit_marker => \$exit_marker,
																max_cycles => $config->{nmisd_worker_max_cycles});
				exit( $status != EXIT_ERROR ? 0 : EXIT_ERROR );
			}
			else											# the parent
			{
				$nmisng->log->debug("started worker process $newpid");
				# starttime is a tad hard to access and just guessing may be a bad idea
				my ($newinfo) = grep($_->pid == $newpid, @{$pt->table}); # there must be exactly one
				$zoo->{$newpid} = { processtype => "worker",
														starttime => $newinfo->start };
			}
		}
	}

	return { success => 1 };
}

# attached to usr1 for more verbosity and usr2 for less
# requires closure for finding the right logger object
sub verbosity_signal_handler
{
	my ($signame, $logger) = @_;

	return if (ref($logger) ne "NMISNG::Log");
	if ($signame =~ /^USR([12])$/)
	{
		# reconfigure yourself for more/less debug
		my $wantmorenoise = ($1 == 1)?  1 : -1;
		$logger->change_level($wantmorenoise);
		my $newlevel = $logger->level;
		my $newdebug = $logger->detaillevel;

		# and log the change at the new level , may look odd but better than no info
		$logger->$newlevel("received SIG$signame, "
											 .($wantmorenoise > 0? "inc":"dec")
											 ."remented verbosity level to $newlevel, debug to $newdebug");
	}
	return;
}

# the main worker loop: claim jobs from the queue and processes them,
# updates opstatus when done, sleeps a little and repeat...
#
# args: nmisng,
#  exit_marker (scalar ref, holds undef or signal name - required)
#  max_cycles (optional, default: unlimited - set to 1 for a single cycle)
#
# returns: EXIT_POLITE if signalled INT, EXIT_TTL if reached max nr of cycles,
# EXIT_IMMEDIATELY if signalled TERM or if the  parent dies,
# EXIT_IDLE if max_cycles was 1 and no jobs were handled,
# EXIT_ERROR for anything else
sub worker_loop
{
	my (%args) = @_;

	my ($nmisng,$max_cycles,$mustdie) = @args{"nmisng","max_cycles","exit_marker"};
	die "invalid arguments, no NMISNG object available!\n" if (ref($nmisng) ne "NMISNG");
	die "invalid arguments, exit_marker is not a ref!\n" if (ref($mustdie) ne "SCALAR");

	my $config = $nmisng->config;
	my $ttl = $max_cycles;
	my $parent = getppid;

	while (!$$mustdie and (!defined($ttl) or $ttl))
	{
		# should terminate if the parent nmisd dies, for any reason
		return EXIT_IMMEDIATELY if (getppid != $parent);

		if (-f "$config->{'<nmis_conf>'}/NMIS_IS_LOCKED")
		{
			$nmisng->log->info("nmis is locked, sleeping 60 seconds");
			sleep(60);
			next;
		}

		# look for and claim the next due worker job from the database, perform the action and opstatus-report result
		my $jobs = $nmisng->get_queue_model(time => { '$lt' => Time::HiRes::time }, # due to start now
																				type => { '$ne' => 'schedule' }, # a job for a worker
																				in_progress => 0, # and not in claimed and in progress yet

																				sort => [ priority => -1, time => 1 ], # first by highest prio, then oldest job
				);
		if (my $error = $jobs->error)
		{
			$nmisng->log->error("Failed to check job queue: $error - sleeping 60 seconds");
			sleep(60);
			next;
		}

		# try to claim one of the N jobs on the menu
		my $ourjob;
		my $feelingsleepy = ($jobs->count < 2); # loop tightly until out of work to do
		for my $canihaveit (@{$jobs->data})
		{
			# mark the job as in-progress and owned by this process,
			# but atomically so as to not interfere with other workers trying the same thing
			$canihaveit->{in_progress} = Time::HiRes::time;
			$canihaveit->{status}->{pid} = $$;

			my ($error, undef) = $nmisng->update_queue(jobdata => $canihaveit,
																								 atomic => { in_progress => 0,
																														 time => $canihaveit->{time} }); # see postpone below
			if ($error)
			{
				# no matching object or the like means this process was preempted
				if ($error =~ /no matching object/i)
				{
					$nmisng->log->debug2("Ignoring Job $canihaveit->{_id}: claimed by other worker");
				}
				else
				{
					$nmisng->log->debug("Could not claim job $canihaveit->{_id}: $error");
				}
				next;
			}

			# a job was claimable, but is it safe to perform this op right now?
			# look for in_progress interfering jobs (needs args check for node-specific stuff!)
			my %checkargs = ( time => { '$lt' => Time::HiRes::time }, # should have started
												in_progress => { '$ne' => 0 },					# has started
												id => { '$ne' => $canihaveit->{_id} },	# is not this job
												type => $canihaveit->{type} );					# but of the same type
			# and concerns the same node as this job
			$checkargs{"args.uuid"} = $canihaveit->{args}->{uuid} if ($canihaveit->{type} =~ /^(collect|update|services)$/);

			my $maybeclash = $nmisng->get_queue_model(%checkargs);
			if (my $error = $maybeclash->error)
			{
				$nmisng->log->error("Failed to check for clashing jobs: $error - assuming no in_progress instances exist");
				$ourjob = $canihaveit;
				last;
			}
			if ($maybeclash->count)
			{
				# possible reactions? a) abort the job completely (until somebody schedules another instance),
				# or b) unclaim the job and postpone it for a few seconds
				my $postpone = $nmisng->config->{postpone_clashing_schedule};
				if ($postpone && $postpone > 0)
				{
					$canihaveit->{in_progress} = 0;
					$canihaveit->{time} += $postpone;
					$canihaveit->{status} = { pid => undef };

					$nmisng->log->warn("An in-progress job clashes with job $canihaveit->{_id}, postponing this job");

					my ($error, undef) = $nmisng->update_queue(jobdata => $canihaveit);
					if ($error)
					{
						$nmisng->log->error("Failed to postpone job $canihaveit->{_id}: $error");
						# we're stuck owning the job that we can't safely process, nothing to be done really
						# safer to terminate the worker than silently abandoning the claimed job
						die("Failed to postpone job $canihaveit->{_id}: $error\n");
					}

					# fixme9 add opstatus
				}
				else
				{
					# abort the job as unprocessable
					# fixme9 report opstatus
					$nmisng->log->warn("An in-progress job clashes with job $canihaveit->{_id}, discarding this job");
					if (my $error = $nmisng->remove_queue(id => $canihaveit->{_id}))
					{
						$nmisng->log->error("Failed to remove job $canihaveit->{_id} from queue: $error");
						die("Failed to remove job $canihaveit->{_id}: $error\n"); # owned but cannot dispose of...
					}
				}
				next;										# try another job, this one wasn't processable
			}

			$ourjob = $canihaveit;
			last;
		}

		# if running in single-op mode but left without any work, say so
		return EXIT_IDLE if ($max_cycles == 1 && !$ourjob);

		# we have some work to do!
		if ($ourjob)
		{
			my ($oldverbosity, $normallog);
			# does the job carry a custom output directive?
			# then TEMPORARILY switch to that
			if (defined(my $logpathprefix = $ourjob->{output}))
			{
				$normallog = $nmisng->log;
				$logpathprefix = File::Spec->rel2abs($logpathprefix, $nmisng->config->{'<nmis_logs>'});

				# expectation is that verbosity is also given
				my $verbosity = $ourjob->{verbosity} // $nmisng->config->{log_level};

				my $logfn = "$logpathprefix-".Time::HiRes::time.".log";
				if (!-d dirname($logfn)) # we don't create new dirs
				{
					$nmisng->log->error("Ignoring custom output directive, directory for file \"$logfn\" does not exist!");
					undef $normallog;			# not switching, so no switching back
				}
				elsif (-f $logfn)				# unlikely but BSTS - don't overwrite/change any existing files, ever
				{
					$nmisng->log->error("Ignoring custom output directive, file \"$logfn\" already exists!");
					undef $normallog;			# not switching, so no switching back
				}
				else
				{
					$nmisng->log->debug("Temporarily switching to custom log file \"$logfn\", verbosity \"$verbosity\" for job $ourjob->{_id}");
					my $thisjoblog = NMISNG::Log->new(level => NMISNG::Log::parse_debug_level(debug => $verbosity),
																						path => $logfn);
					$thisjoblog->logprefix($nmisng->log->logprefix); # do keep the logprefix
					$nmisng->log($thisjoblog);
				}
			}
			# ordoes the job carry a custom verbosity flag? then apply that temporarily
			elsif (defined(my $joblevel = $ourjob->{verbosity}))
			{
				$oldverbosity = $nmisng->log->new_level($joblevel);
			}
			$nmisng->log->info("Processing job $ourjob->{_id}, type $ourjob->{type}"
												 .($ourjob->{tag}? ", tag \"$ourjob->{tag}\"":""));


			my $state = { stats => { time => Time::HiRes::time }};		# for opstatus and exception handling
			$0 = "nmisd." . $ourjob->{type}; # Change worker name
			
			# the simpler jobs don't have extra args that need to go into opstatus context
			if ($ourjob->{type} =~ /^(selftest|permissions_test|configbackup|purge|dbcleanup|escalations|thresholds|metrics)$/)
			{
				(my $error, $state->{statusid}) =
						$nmisng->save_opstatus(activity => $ourjob->{type},
																	 status => "inprogress",
																	 type => "start",
																	 details => "Worker $$ starting operation",
																	 context => { queue_id => $ourjob->{_id},
																								queue_tag => $ourjob->{tag},
																								worker_process => $$ },
						);
				$nmisng->log->error("Failed to save opstatus: $error") if ($error);
			}

			# we don't want to end up with stuck jobs if anything dies, so bsts
			try
			{
				my $meta = $ourjob->{args}->{meta};
				if ($ourjob->{type} eq "selftest" or $ourjob->{type} eq "permission_test")
				{
					my ($allok, $tests) = NMISNG::Util::selftest(nmisng => $nmisng,
																											 delay_is_ok => 1,
																											 perms => ($ourjob->{type} eq "permission_test") );

					my $failedtests = join(", ", map { $_->[0] } (grep(defined $_->[1], @$tests)));

					$state->{status} = ($allok?"ok":"error");
					$state->{details} = "completed"
							.($allok? " successfully" : " with failed tests: $failedtests");

					# on failure raise a stateless event (if possible, ie. if localhost node exists)
					if (!$allok)
					{
						my $S = NMISNG::Sys->new(nmisng => $nmisng);
						if ($S->init(name => "localhost", snmp => 'false', wmi => 'false'))
						{
							Compat::NMIS::notify(sys => $S, event => "Selftest Failed",
																	 details => "failed tests: $failedtests");
						}
					}
				}
				elsif ($ourjob->{type} eq "configbackup")
				{
					my $result = $nmisng->config_backup;
					$state->{status} = ($result->{success}? "ok":"error");
					$state->{details} = "completed"
							.($result->{success}? " successfully, backup saved as $result->{file}"
								: "failed: $result->{error}"),
				}
				elsif ($ourjob->{type} eq "purge")
				{
					my $files_res = $nmisng->purge_old_files(simulate => 0);
					my $outages_res = NMISNG::Outage::purge_outages(nmisng => $nmisng,
																													 simulate => 0);
					my $totalsuccess = $files_res->{success} && $outages_res->{success};
					my $errors = join("\n", grep(defined $_, $files_res->{error}, $outages_res->{error}));
					my $allinfos = join("\n", "", grep(defined $_, @{$files_res->{info}},
																						 $outages_res->{message}, @{$outages_res->{info}}));

					$state->{status} = $totalsuccess? "ok":"error";
					$state->{details} => "completed"
							.( $totalsuccess? " successfully" : "failed: $errors").$allinfos,
				}
				elsif ($ourjob->{type} eq "dbcleanup")
				{
					# for the dbcleanup create a new nmisng object with drop_unwanted_indices set to true
					$nmisng = NMISNG->new(config => $nmisng->config,
																log => $nmisng->log,
																drop_unwanted_indices => 1);

					my $result = $nmisng->dbcleanup(simulate => 0);
					$state->{status} = $result->{success}? "ok":"error";
					$state->{details} = "completed"
							.($result->{success}? " successfully" : "failed: $result->{error}")
							.join("\n", "", @{$result->{info}});
				}
				elsif ($ourjob->{type} eq "escalations")
				{
					# fixme: does not return anything
					$nmisng->process_escalations;
					$state->{status} = "ok";
					$state->{details} = undef;
				}
				elsif ($ourjob->{type} eq "thresholds")
				{
					# fixme: does not return anything
					$nmisng->compute_all_thresholds;
					$state->{status} = "ok";
					$state->{details} = undef;
				}
				elsif ($ourjob->{type} eq "metrics")
				{
					my $result = $nmisng->compute_metrics;
					$state->{status} = $result->{success}? "ok":"error";
					$state->{details} = $result->{success}? "completed successfully" : "failed: $result->{error}";
				}
				elsif ($ourjob->{type} eq "delete_nodes")
				{
					my $uuid = $ourjob->{args}->{uuid};
					my $name = $ourjob->{args}->{node};
					my $nodes = "";
					my $error = 0;
					my $errormsg = "";
					my $keeprrds = $ourjob->{args}->{keeprrds} // 0;
					
					my $nodemodel = $nmisng->get_nodes_model(filter => {name => $name, uuid => $uuid});

					if (!$nodemodel->count) {
						$nmisng->log->error("No matching nodes exist");
						$error = 1;
					} else {
						my $gimmeobj = $nodemodel->objects; # instantiate, please!
						if (!$gimmeobj->{success}) {
							$nmisng->log->error("Failed to instantiate node objects: $gimmeobj->{error}");
							$error = 1;
						} else {
							for my $mustdie (@{$gimmeobj->{objects}})
							{
								# First, backup
								my $backup = $config->{'backup_node_on_delete'} // 1;
								my $backup_folder = $config->{'node_dumps_dir'} // $config->{'<nmis_var>'}."/node_dumps";
								if ( !-d $backup_folder )
								{
									mkdir( $backup_folder, 0700 ) or return {error => "Cannot create $backup_folder: $!"};
								}
								my $res;
								
								if ($backup) {
									$res = $nmisng->dump_node(name => $mustdie->name,
														 uuid => $mustdie->uuid,
														 target => $backup_folder . "/".$mustdie->name.".zip",
														 setperms => 1, override => 1);
														 #options => \%options);
								}
								if (!$backup || $res->{success}) {
									
									(my $ok, $errormsg) = $mustdie->delete(keep_rrd => $keeprrds, meta => $meta); # === eq false
									$nmisng->log->debug("********* $ok deleting node ". $mustdie->name);
									if (!$ok) {
										$nmisng->log->error("Error deleting node: $errormsg");
										$error = 1;
									} else {
										$nodes = $nodes . " " . $mustdie->name;
									}
								} else {
									$nmisng->log->error("Failed to backup node: ". $mustdie->name);
									$error = 1;
								}	
							}
						}
					}
					if ($error == 0) {
						$state->{status} = "ok";
						$state->{details} = "completed successfully";
					} else {
						$state->{status} = "error";
						$state->{details} = "Error deleting node $errormsg";
					}
					
					$state->{context} = {node_name => $nodes};
				}
				elsif ($ourjob->{type} eq "create_nodes" || $ourjob->{type} eq "update_nodes")
				{
					my $nodes = "";
					my $error = 0;
					my $errormsg = "";
					my $what = $ourjob->{type};
					
					my $mayberec = $ourjob->{args}->{data};
					
					my @nodes;
					
					if (ref($mayberec) eq "HASH") {
						push @nodes, $mayberec;
					}
					foreach my $n (@$mayberec)  {
							push @nodes, $n;
					}
					
					
					foreach my $n (@nodes) {
						my %query = $n->{uuid}? (uuid => $n->{uuid}) : (name => $n->{name});
						my $nodeobj = $nmisng->node(%query);
						$nodeobj ||= $nmisng->node(uuid => $n->{uuid}, create => 1);
					
						$nodes = $nodes . " " . $n->{name};
						
						if (ref($nodeobj) ne "NMISNG::Node") {
							$nmisng->log->error("Failed to instantiate node object!\n");
							$error = 1;
						} else {
							my $isnew = $nodeobj->is_new;
							# must set overrides, activated, name/cluster_id, addresses/aliases, configuration;
							for my $mustset (qw(cluster_id name activated overrides configuration addresses aliases))
							{
								$nodeobj->$mustset($n->{$mustset}) if (exists($n->{$mustset}));
								delete $n->{$mustset};
							}
							# if creating, add missing cluster_id for local operation
							$nodeobj->cluster_id($config->{cluster_id}) if (!$nodeobj->cluster_id);
							# there should be nothing left at this point, anything that is goes into unknown()
							my %unknown = map { ($_ => $n->{$_}) } (grep(!/^(configuration|lastupdate|uuid)$/, keys %$n));
							$nodeobj->unknown(\%unknown);
							
							my ($status,$msg) = $nodeobj->save(meta => $meta);
							
							# zero is no saving needed, which is not good here
							if ($status <= 0) {
								$nmisng->log->error("failed to ".($isnew? "create":"update")." node $n->{uuid}: $msg\n");
								$error = 1;
							}						
						}

					}
					
					if ($error == 0) {
						$state->{status} = "ok";
						$state->{details} = "completed successfully ";
					} else {
						$state->{status} = "error";
						$state->{details} = "Error $what node $errormsg";
					}
					
					$state->{context} = {node_name => $nodes};	
				}
				elsif ($ourjob->{type} eq "set_nodes" ) {
					my $nodes_list = "";
					my $error = 0;
					my $errormsg = "";
					my $what = $ourjob->{type};
					
					my $data = $ourjob->{args}->{data};
					my $nodes = $ourjob->{args}->{uuid};
					
					if (exists($data->{"entry.name"})) {
							$nmisng->log->error("set_nodes: Please use act=rename for node renaming! ");
							$error = 1;
							next;
					} else {
						foreach my $n (@$nodes) {
							$nmisng->log->error("set_nodes: Processing node $n! ");
							my $nodeobj = $nmisng->node(name => $n) || $nmisng->node(uuid=> $n);
							if (!$nodeobj) {
								$error =1;
								next;
							}
							
							my $node = $nodeobj->name;			# if looked up via uuid
						
							my $curconfig = $nodeobj->configuration;
							my $curoverrides = $nodeobj->overrides;
							my $curactivated = $nodeobj->activated;
							my $curextras = $nodeobj->unknown;
							my $curarraythings = { aliases => $nodeobj->aliases,
																		 addresses => $nodeobj->addresses };
							my $anythingtodo;
				
							for my $name (keys %$data)
							{
								# . are replaced by _ for the queue
								my $prop = $name;
								$prop =~ s/_/./g;
								
								next if ($prop !~ /^entry\./); # we want only entry.thingy, so that act= and debug= don't interfere
								++$anythingtodo;
						
								my $value = $data->{$name};
								undef $value if ($value eq "undef");
								$prop =~ s/^entry\.//;
						
								# translate the backwards-compatibility configuration.active, which shadows activated.NMIS
								$prop = "activated.NMIS" if ($prop eq "configuration.active");
						
								# where does it go? overrides.X is obvious...
								if ($prop =~ /^overrides\.(.+)$/)
								{
									$curoverrides->{$1} = $value;
								}
								# ...name, cluster_id a bit less...
								elsif ($prop =~ /^(name|cluster_id)$/)
								{
									$nodeobj->$1($value);
								}
								# ...and activated.X not at all
								elsif ($prop =~ /^activated\.(.+)$/)
								{
									$curactivated->{$1} = $value;
								}
								# ...and then there's the unknown unknowns
								elsif ($prop =~ /^unknown\.(.+)$/)
								{
									$curextras->{$1} = $value;
								}
								# and aliases and addresses, but these are ARRAYS
								elsif ($prop =~ /^((aliases|addresses)\.(.+))$/)
								{
									$curarraythings->{$1} = $value;
								}
								# configuration.X
								elsif ($prop =~ /^configuration\.(.+)$/)
								{
									$curconfig->{$1} = $value;
								}
								else
								{
									die "Unknown property \"$prop\"!\n";
								}
							}
							if (!$anythingtodo) {
								$nmisng->log->error("set_nodes: No changes for node \"$node\"!");
								$error = 1;
								next;
							}
						
							for ([$curconfig, "configuration"],
									 [$curoverrides, "override"],
									 [$curactivated, "activated"],
									 [$curarraythings, "addresses/aliases" ],
									 [$curextras, "unknown/extras" ])
							{
								my ($checkwhat, $name) = @$_;
						
								my $error = NMISNG::Util::translate_dotfields($checkwhat);
								if ($error) {
									$nmisng->log->error("set_nodes: translation of $name arguments failed: $error!");
									$error = 1;
									next;
								}
							}
						
							$nodeobj->overrides($curoverrides);
							$nodeobj->configuration($curconfig);
							$nodeobj->activated($curactivated);
							$nodeobj->addresses($curarraythings->{addresses});
							$nodeobj->aliases($curarraythings->{aliases});
							$nodeobj->unknown($curextras);
						
							(my $op, $error) = $nodeobj->save(meta => $meta);
							if ($op <= 0) {
									$nmisng->log->error("set_nodes: Failed to save $node: $error");
									$error = 1;
									next;
								}
							$nodes_list = $nodes_list . " " . $node;
	
						}
					}
					
					if ($error == 0) {
						$state->{status} = "ok";
						$state->{details} = "completed successfully ";
					} else {
						$state->{status} = "error";
						$state->{details} = "Error setting node $errormsg";
					}
					
					$state->{context} = {node_name => $nodes_list};	
				}
				elsif ($ourjob->{type} eq "plugins")
				{
					# run after-collect/after-update plugins once all node jobs are done
					# required args: phase and uuid (array)
					my $phase = $ourjob->{args}->{phase};
					my $uuids = $ourjob->{args}->{uuid};
					if ($phase !~ /^(update|collect)$/
							or ref($uuids) ne "ARRAY" or !@$uuids)
					{
						$state->{status} = "error";
						$state->{details} = "aborted: invalid arguments for plugin invocation!";
					}
					else
					{
						$nmisng->log->info("Starting post-${phase} plugins for "
															 .scalar(@$uuids)." nodes, tag was \"$ourjob->{tag}\"");

						# lookup the node names so that we can provide both names
						# and uuids in the same order
						my $u2n = $nmisng->get_nodes_model(uuid => $uuids,
																							 fields_hash => { uuid => 1, name => 1 });
						my (@node_names, @node_uuids);
						map { push @node_names, $_->{name}; push @node_uuids, $_->{uuid}; } (@{$u2n->data});

						$state->{activity} = "post-${phase} $ourjob->{type}";
						# we'd like the opstatus to carry a useful context,
						# so try to include some/all node uuids (fixme9: index too big errors can occur)
						(my $error, $state->{statusid}) =
								$nmisng->save_opstatus(activity => $state->{activity},
																			 status => "inprogress",
																			 type => "start",
																			 details => "Worker $$ starting operation",
																			 context => { queue_id => $ourjob->{_id},
																										queue_tag => $ourjob->{tag},
																										worker_process => $$,
																										node_uuid => \@node_uuids,
																										node_name => \@node_names },
								);
						$nmisng->log->error("Failed to save opstatus: $error") if ($error);

						my $S;
						for my $plugin ($nmisng->plugins)
						{
							my $funcname = $plugin->can("after_${phase}_plugin");
							next if ( !$funcname );

							if (!$S)
							{
								$S = NMISNG::Sys->new(nmisng => $nmisng);    # the nmis-system object
								$S->init();
							}

							$nmisng->log->debug2("Running after_${phase} plugin $plugin");
							my $prevprefix = $nmisng->log->logprefix;
							$nmisng->log->logprefix("$plugin\[$$\] ");
							++$state->{stats}->{plugins};

							my ($status, @errors, $fatality);
							# we can't trust the plugins fully, so...
							try
							{
								( $status, @errors ) = &$funcname( sys => $S,
																									 config => $nmisng->config,
																									 nodes => \@node_names,
																									 nodes_uuids => \@node_uuids,
																									 nmisng => $nmisng  );
							}
							catch
							{
								$fatality = $_;
							};

							$nmisng->log->logprefix($prevprefix);
							if ($status >= 2 or $status < 0 or $fatality)
							{
								$state->{status} = "error";
								$state->{problems} //= [];
								++$state->{stats}->{pluginfaults};
								push @{$state->{problems}}, "Plugin $plugin failed to run: $fatality" if ($fatality);
								push @{$state->{problems}}, @errors;

								$nmisng->log->error("Plugin $plugin failed to run: $fatality") if ($fatality);
								for my $err (@errors)
								{
									$nmisng->log->error("Plugin $plugin: $err");
								}
							}
							else
							{
								$nmisng->log->debug("Plugin $plugin "
																		. ($status? "indicated success" : "indicated no changes"));
							}
						}

						$state->{status} ||= "ok";
						$state->{details} = "completed ". (ref($state->{problems}) eq "ARRAY"?
																							 ("with errors: ".join("\n",
																																		 @{$state->{problems}})) : "successfully");
					}
				}
				elsif ($ourjob->{type} =~ /^(collect|update|services)$/)
				{
					# required: args with uuid of node to work on,
					# optional force
					# collect: also requires wantsnmp and wantwmi
					# services: also allows services (=list of preselected service names)
					my $nodeuuid = $ourjob->{args}->{uuid};

					my $methodname = $ourjob->{type};
					my @methodargs;
					if ($ourjob->{type} eq "collect")
					{
						@methodargs = ( wantsnmp => $ourjob->{args}->{wantsnmp},
														wantwmi => $ourjob->{args}->{wantwmi} );
					}
					elsif ($ourjob->{type} eq "services")
					{
						@methodargs = ( services => $ourjob->{args}->{services} );
					}
					push @methodargs, (force => 1) if ($ourjob->{args}->{force});

					my $maxruntime = defined($config->{max_child_runtime})
							&& $config->{max_child_runtime} > 0 ?
							$config->{max_child_runtime} : 0;
					alarm($maxruntime) if ($maxruntime);

					my $thenode = $nmisng->node(uuid => $nodeuuid);
					die("Invalid arguments for $methodname: no node with uuid $nodeuuid exists!\n")
							if (!$thenode or $thenode->is_new);

					# we'd like the opstatus to carry a useful context
					(my $error, $state->{statusid}) =
							$nmisng->save_opstatus(activity => $ourjob->{type},
																		 status => "inprogress",
																		 type => "start",
																		 details => "Worker $$ starting operation for ".$thenode->name,
																		 context => { queue_id => $ourjob->{_id},
																									queue_tag => $ourjob->{tag},
																									worker_process => $$,
																									node_uuid => [ $nodeuuid ],
																									node_name => [ $thenode->name ],
																									output => $ourjob->{output}},
							);
					$nmisng->log->error("Failed to save opstatus: $error") if ($error);

					$nmisng->log->info("Starting $methodname for node $nodeuuid (".$thenode->name.")");
					my $result = $thenode->$methodname(@methodargs);
					alarm(0) if ($maxruntime);
					$nmisng->log->debug2("Finished $methodname for node $nodeuuid (".$thenode->name.")");

					$nmisng->log->debug(sprintf("$methodname for node $nodeuuid (%s) took %.1fs",
																			$thenode->name, (Time::HiRes::time - $state->{stats}->{time})));

					# locked is not-quite an error, so we report it as informational
					$state->{status} = $result->{success}? "ok": $result->{locked}? "info" : "error";
					$state->{details} = $result->{success}? "completed successfully (".$thenode->name.")" : "failed: $result->{error}";
				}
				else
				{
					# fixme how to handle unsupported job types?
					croak("unsupported job type in ".Dumper($ourjob));
				}
			}
			catch
			{
				$state->{exception} = $_;
			};

			$0 = "nmisd.worker.<idle>";			# some of the ops rename the process temporarily

			if ($state->{exception})
			{
				$nmisng->log->fatal("Exception while processing job $ourjob->{_id}: $state->{exception}");
				# don't care about success or failure of that here
				$nmisng->save_opstatus(id => $state->{statusid},
															 activity => $state->{activity} // $ourjob->{type},
															 status => "error",
															 type => "exception",
															 details => $state->{exception},
															 stats => undef,
															 context => $state->{context});
			}
			else
			{
				# work is done, report result in opstatus
				$state->{stats}->{time} = Time::HiRes::time - $state->{stats}->{time}; # time was start marker, want it to be delta
				my ($error, undef) = $nmisng->save_opstatus(id => $state->{statusid},
																										activity => $state->{activity} // $ourjob->{type},
																										type => "completed",
																										status => $state->{status},
																										details => $state->{details},
																										stats => $state->{stats},
																										context => $state->{context},
															 );
				$nmisng->log->error("Failed to save opstatus: $error") if ($error);

				# and convenience-log the time it's taken (for per-node jobs that's already done with more context)
				$nmisng->log->debug(sprintf("$ourjob->{type} took %.1fs", $state->{stats}->{time}))
						if ($ourjob->{type} !~ /^(collect|update|services)$/);
			}

			# and delete the job from the queue
			$nmisng->log->info("Completed job $ourjob->{_id}, $ourjob->{type}, removing from queue");
			if (my $error = $nmisng->remove_queue(id => $ourjob->{_id}))
			{
				$nmisng->log->error("Failed to remove job $ourjob->{_id} from queue: $error");
			}

			# restore the normal long-term logger and ditch the
			if (defined $normallog)
			{
				my $templog = $nmisng->log;
				close $templog->handle if ($templog->handle);

				$nmisng->log($normallog);
			}
			elsif (defined $oldverbosity)
			{
				# or restore the original verbosity level if there was a per-job one
				$nmisng->log->new_level($oldverbosity)
			}
		}

		# sleep if there's no work left to do, and no cycle limit or not the final cycle yet
		if ($feelingsleepy and (!defined($ttl) or $ttl > 1))
		{
			# we want to react to SIGURG only while idle
			$SIG{"URG"} = sub { (my $sig) = @_; $nmisng->log->debug2("received $sig"); };
			my $catnap = $config->{nmisd_worker_cycle} // 30;
			$nmisng->log->debug2("Sleeping for up to $catnap seconds");
			sleep($catnap);
			$SIG{"URG"} = 'IGNORE';
		}
		--$ttl if (defined($ttl) && $ttl > 0);

	}
	return defined($$mustdie)? $$mustdie eq "INT"? EXIT_POLITE : EXIT_IMMEDIATELY
			: $max_cycles? EXIT_TTL : EXIT_ERROR;
}

# the dedicated fping worker loop: ping nodes in need of,
# update status file, sleep a little and repeat...
#
# args: nmisng,
#  exit_marker (scalar ref, holds undef or signal name - required)
#  max_cycles (optional, default: unlimited - set to 1 for a single cycle)
#
# returns: EXIT_POLITE if signalled INT, EXIT_TTL if reached max nr of cycles,
# EXIT_IMMEDIATELY if signalled TERM or if the parent dies,
# EXIT_IDLE if max_cycles was 1 and no jobs were handled,
# EXIT_ERROR for anything else
sub fping_loop
{
	my (%args) = @_;

	my ($nmisng,$max_cycles,$mustdie) = @args{"nmisng","max_cycles","exit_marker"};
	die "invalid arguments, no NMISNG object available!\n" if (ref($nmisng) ne "NMISNG");
	die "invalid arguments, exit_marker is not a ref!\n" if (ref($mustdie) ne "SCALAR");

	my $config = $nmisng->config;
	my $ttl = $max_cycles;

	my $fpingpath4;
	my $fpingpath6;

	# new fping options for "interval between sending ping packets" and "interval between ping packets to one target"
	# some crazy firewalls like to have larger gaps, so -i 80 and -p 100 could be used in the config.
	my $interval = $config->{fastping_interval} || 1;
	my $target_interval   = $config->{fastping_target_interval} || 1;

	# need to set these as well, but never less than the limits for non-root users.
	my $non_root_interval = $config->{fastping_interval} || 10;
	my $non_root_target_interval   = $config->{fastping_target_interval} || 20;
	$non_root_interval = 10 if $non_root_interval < 10;
	$non_root_target_interval = 20 if $non_root_interval < 20;

	# OLD CODE  #############
	# min packet size is 20+4 bytes, default is 56
	# ($config->{fastping_packet} ? $config->{fastping_packet} >= 24 ? $config->{fastping_packet} : 24 : 56)
	#############

	# ICMP Packet sizes are data + 8 bytes of ICMP + 20 bytes of IP + 14 bytes of ethernet 
	# todo, should we log a warning if someone configures very large or very small packet sizes for ICMP?
	my $fastping_packet = $config->{fastping_packet};

	# we want fping to do the work, no need to avg/min/max ourselves
	# -c X produces: 'somenode : xmt/rcv/%loss = 5/5/0%, min/avg/max = 0.73/0.86/0.97'
	my @fpingargs = (
				"-t", ($config->{fastping_timeout} || 300),
				"-c", ($config->{fastping_count} || 3),
				"-q",
				"-r", ($config->{fastping_retries} || 3),
				"-b", $fastping_packet 
			);

	push @fpingargs, ($< == 0?
				# we may use these options if we run as root
				("-i", $interval, "-p", $target_interval)
				: # non-root requires i >= 10, p >= 20, r < 20, and t >= 50
				("-i", $non_root_interval, "-p", $non_root_target_interval)
			);

	my %state;
	my $parent = getppid;

	while (!$$mustdie and (!defined($ttl) or $ttl))
	{
		# should terminate if the parent nmisd dies, for any reason
		return EXIT_IMMEDIATELY if (getppid != $parent);

		if (-f "$config->{'<nmis_conf>'}/NMIS_IS_LOCKED")
		{
			$nmisng->log->info("nmis is locked, sleeping 60 seconds");
			sleep(60);
			next;
		}
		# on first startup, find the fping executable
		if (!defined $fpingpath4)
		{
			for my $pathcomp (split(/:/, $ENV{PATH}))
			{
				if (-x(my $maybe = "$pathcomp/fping"))
				{
					$fpingpath4 = $maybe;
					last;
				}
			}
			if (!defined $fpingpath4)
			{
				$nmisng->log->fatal("no fping executable available! sleeping 60 seconds");
				sleep(60);
				next;
			}
		}
		if (!defined $fpingpath6)
		{
			for my $pathcomp (split(/:/, $ENV{PATH}))
			{
				if (-x(my $maybe = "$pathcomp/fping6"))
				{
					$fpingpath6 = $maybe;
					last;
				}
			}
			if (!defined $fpingpath6)
			{
				$nmisng->log->fatal("no fping executable available! sleeping 60 seconds");
				sleep(60);
				next;
			}
		}

		# who needs pinging?
		# polling-policy: reread every cycle, cached
		my $policies = NMISNG::Util::loadTable(dir => 'conf', name => "Polling-Policy") || {};

		# translate unitised values into raw seconds
		for my $polname (keys %$policies)
		{
			next if (ref($policies->{$polname}) ne "HASH"); # bsts
			for my $mayhaveunit (qw(ping))											# we only need the ping policy interval here
			{
				my $interval = $policies->{$polname}->{$mayhaveunit} // 60; # seconds
				# supports NNN (seconds) or MMMU with U being s, m, h or d, fractional NNN or MMM also ok
				if ($interval =~ /^\s*(\d+(\.\d+)?)([smhd])$/)
				{
					my ($rawvalue, $unit) = ($1, $3);
					$interval = $rawvalue * ($unit eq 'm'? 60 : $unit eq 'h'? 3600 : $unit eq 'd'? 86400 : 1);
				}
				$policies->{$polname}->{$mayhaveunit} = $interval;
			}
		}

		my $nodesmodel = $nmisng->get_nodes_model( filter => { 	cluster_id => $config->{cluster_id},
																														"activated.NMIS" => 1,
																														"configuration.ping" => 1 });
		if (my $failure = $nodesmodel->error)
		{
			$nmisng->log->error("Failed to query active nodes: $failure, sleeping 60s");
			sleep(60);
			next;
		}

		my $now = Time::HiRes::time;
		my $allnodes = $nodesmodel->data; # array of node config records
		my %byuuid = map { $_->{uuid} =>  $_} (@$allnodes); # state is kept by uuid

		# first: ditch deleted/disabled nodes
		for my $maybegoner (keys %state)
		{
			my ($uuid,$declash) = split(/:/, $maybegoner);

			delete $state{$maybegoner}
			if (ref($byuuid{$uuid}) ne "HASH");
		}
		# the same needs to happen for no-longer multi-homed nodes
		for my $maybegoner (grep(defined($state{$_}->{has_sibling}),
														 keys %state))
		{
			delete $state{$maybegoner}
			if (!$byuuid{ $state{$maybegoner}->{uuid} }->{host_backup});
		}

		# second: find out which nodes are due for a ping
		my (@todos, %multihomed);
		my @tocheck = values %byuuid;

		while (my $noderec = pop @tocheck)
		{
			# if a node is dual-homed then it needs to be pinged twice, so two records with two keys...
			my $statekey = $noderec->{uuid};
			if ($noderec->{configuration}->{host_backup})
			{
				# first time round, handle primary and add the secondary
				if (!$multihomed{$noderec->{uuid}})
				{
					$multihomed{$noderec->{uuid}} = 1;
					push @tocheck, $noderec;
					$statekey .= ":0";
					$nmisng->log->debug("Node $noderec->{name} is multi-homed, will ping both $noderec->{configuration}->{host} and $noderec->{configuration}->{host_backup}");				}
				# second time round, do handle the secondary
				else
				{
					$noderec = Clone::clone($noderec); # no overwriting of the original record!
					$noderec->{configuration}->{host} = $noderec->{configuration}->{host_backup};
					$statekey .= ":1";
				}
			}

			my $thisstate = $state{ $statekey } ||= {
				name => $noderec->{name},
				uuid => $noderec->{uuid},
				host => $noderec->{configuration}->{host},       # this is either the  primary or the secondary host/ip
				policy => $noderec->{configuration}->{polling_policy} || 'default',
			};
			# make sure host info isn't stale, ever (e.g. node with host field == ip address, which is then changed)
			$thisstate->{host} = $noderec->{configuration}->{host};

			if (exists($multihomed{$thisstate->{uuid}})) # temporary property is present for both
			{
				$thisstate->{has_sibling} = $thisstate->{uuid};
				$thisstate->{is_primary} = $statekey =~ /:0$/?1:0;
			}

			$thisstate->{ip_protocol} = $noderec->{configuration}->{ip_protocol};
			
			# honor fixed ip address given
			if (Net::IP::ip_is_ipv4($thisstate->{host}) || Net::IP::ip_is_ipv6($thisstate->{host}))
			{
				$thisstate->{ip} = $thisstate->{host};
			}
			# allowed to resolve name and cache address?
			elsif (!NMISNG::Util::getbool($config->{fastping_cache_dns},"invert"))
			{
				if (!$thisstate->{ip} or $now >= $thisstate->{nextdns})
				{
					my $ip;
					if ($thisstate->{ip_protocol} eq 'IPv6')
					{
						$ip = NMISNG::Util::resolveDNStoAddrIPv6($thisstate->{host});
					}
					else
					{
						$ip = NMISNG::Util::resolveDNStoAddr($thisstate->{host});
					}

					$thisstate->{ip} = $ip # may be undef
							||  $thisstate->{host}; # again falling back to leaving this to fping
					$nmisng->log->debug2("refreshed dns for $thisstate->{host} to $thisstate->{ip}"
															 .($thisstate->{nextdns}?
																 sprintf(", was due %.2fs ago", $now - $thisstate->{nextdns}):""));
					$thisstate->{nextdns} = $now + 900; # dns-sourced addresses may be remembered for 15 minutes
				}
			}
			# otherwise leave it to fping to Do Something with the host (name or whatever)
			else
			{
				$thisstate->{ip} = $thisstate->{host};
			}

			if (!$thisstate->{nextping} or $thisstate->{nextping} <= $now)
			{
				$nmisng->log->debug2("will ping node $thisstate->{uuid} ($thisstate->{name}, $thisstate->{host}) this cycle"
														 . ($thisstate->{nextping}?
																sprintf(", was due %.2fs ago", $now - $thisstate->{nextping}): "" ));
				push @todos, $statekey;
				$thisstate->{pending} = 1;
			}
		}

		# third: ping the ones in need, in chunks if necessary
		my @thistime = @todos;				# todos is consumed
		while (my @chunk = splice(@todos, 0, ($config->{fastping_node_poll} || 200)))
		{
			my %ip2staterec;
			my @cmd4 = ($fpingpath4, @fpingargs);
			my @cmd6 = ($fpingpath6, @fpingargs);
			my ($hasip4, $hasip6);
			
			for my $statekey (@chunk)
			{
				my $thisip = $state{$statekey}->{ip};
				if ($state{$statekey}->{ip_protocol} eq 'IPv6')
				{
					push @cmd6, $thisip;
					$hasip6 = 1;
				}
				else
				{
					push @cmd4, $thisip;
					$hasip4 = 1;
				}

				$ip2staterec{$thisip} //= [];
				push @{$ip2staterec{$thisip}}, $statekey;
			}
			
			my @cmd;
			push @cmd, ("(");
			push @cmd, @cmd4 if ($hasip4);
			push @cmd, (";") if ($hasip4 && $hasip6);
			push @cmd, @cmd6 if ($hasip6);
			push @cmd, (")");

			$nmisng->log->debug("about to run: ".join(" ",@cmd));

			# pity that fping reports on stderr, NOT stdout; so we need
			# the shell redirection (or a child and replumb)
			if (!open(FROMFPING, "-|", join(" ", @cmd, "2>&1")))
			{
				$nmisng->log->fatal("failed to run fping: $!");
				die "Failed to run fping: $!\n";
			}

			while (my $line = <FROMFPING>)
			{
				chomp $line;
				# goodnode : xmt/rcv/%loss = 5/5/0%, min/avg/max = 0.89/0.96/1.05
				# badnode : xmt/rcv/%loss = 5/0/100%
				# or weirdnode : xmt/rcv/%return = 2/64/3200%, min/avg/max = 0.49/7.39/8.13
				# or nothing for unresolvable.
				$nmisng->log->debug3("fping returned: $line");

				my ($hostnameorip,$dupsorloss,$loss,$min,$avg,$max) = ($1,$2,$3,$4,$5,$6,$7)
						if ($line =~ m!^\s*(\S+)\s*:\s*xmt/rcv/%(loss|return)\s*=\s*\d+/\d+/(\d+)%(?:,\s*min/avg/max\s*=\s*(\d+(?:\.\d+)?)/(\d+(?:\.\d+)?)/(\d+(?:\.\d+)?))?$!);
				if ($dupsorloss eq "return")
				{
					$nmisng->log->warn("fping reports ICMP packet duplication: \"$line\"");
					$loss = 0;
				}

				if ($hostnameorip								# parseable?
						&& $ip2staterec{$hostnameorip} # known?
						&& $loss =~ /^\d+$/ )		  # structure good enough for the reachability at least?
				{
					for my $owner (@{$ip2staterec{$hostnameorip}})
					{
						my $thisstate = $state{$owner};
						# what does the policy say about the interval?
						my $interval = ref($policies->{ $thisstate->{policy} }) eq "HASH"?
								$policies->{ $thisstate->{policy} }->{ping} : 60; # seconds

						# the regex extraction sets missing to blank string, would prefer undef or number
						$thisstate->{loss} = int($loss);
						$thisstate->{avg} = $avg ne ""? 0+$avg : undef;
						$thisstate->{min} = $min ne ""? 0+$min : undef;
						$thisstate->{max} = $max ne ""? 0+$max : undef;

						$thisstate->{lastping} = $now;
						$thisstate->{nextping} = $now + $interval;
						delete $thisstate->{pending};

						$nmisng->log->debug3("parsed result for node $hostnameorip: "
																 .Dumper($thisstate));
					}
				}
				else
				{
					$nmisng->log->error("fping result \"$line\" was not parseable!");
				}
			}
			close FROMFPING;
		}

		# fourth: analyse the results we've got this time, trigger events if required
		for my $statekey (@thistime)
		{
			my $thisstate = $state{$statekey};
			my $uuid = $thisstate->{uuid};

			# save the statistics/results first - for/with the primary
			my $timeddata = {
				min_rtt => $thisstate->{min},
				avg_rtt => $thisstate->{avg},
				max_rtt => $thisstate->{max},
				loss => $thisstate->{loss},
				ip => $thisstate->{ip},
			};
			if ($thisstate->{has_sibling} && $thisstate->{is_primary})
			{
				my $secondary = List::Util::first { $_->{has_sibling} eq $thisstate->{has_sibling}
																						&& !$_->{is_primary} } (values %state);

				$timeddata->{backup_min_rtt} = $secondary->{min};
				$timeddata->{backup_avg_rtt} = $secondary->{avg};
				$timeddata->{backup_max_rtt} = $secondary->{max};
				$timeddata->{backup_loss} = $secondary->{loss};
				$timeddata->{backup_ip} = $secondary->{ip};
			}
			my $node = $nmisng->node(uuid => $uuid);

			# no answers whatsoever? then complain, we don't know if up or down so can't really raise any events
			if ($thisstate->{pending})
			{
				$nmisng->log->error("fping has not reported any state for Node $thisstate->{name} ($thisstate->{ip})!");
				next;
			}
			# backup address goes with the primary
			elsif (!$thisstate->{has_sibling} || $thisstate->{is_primary})
			{
				# get the node's ping inventory, and add timed data for it
				# timed data is only possible for a particular inventory.
				# but even with separate subconcepts, timed data cannot be saved 'incrementally'
				# result: the collect code and this fping code cannot safely share
				# the catchall inventory's timed data
				my ($pinginv,$error) = $node->inventory(concept => "ping", create => 1,
																								data => { }, path_keys => []); # not indexed, one per node
				if ($error or !$pinginv)
				{
					$nmisng->log->error("Failed to instantiate ping inventory: $error");
				}
				else
				{
					$pinginv->save if ($pinginv->is_new); # timed data only possible once inventory is in the db

					# this saves both timed data and the inventory
					my $error = $pinginv->add_timed_data(
						time => $thisstate->{lastping},
						data => $timeddata,
						derived_data => {},
						subconcept => "ping",
							);
					$nmisng->log->error("Failed to add ping timed data: $error") if ($error);
				}
			}

			# now raise events when required, ie. new state
			# note that multi-homed nodes are down IFF all ips are unreachable
			if (!exists($thisstate->{loss}) or $thisstate->{loss} == 100)
			{
				my ($whatevent, $details);
				my $fpingmsg = $thisstate->{loss}? "fping reported loss=$thisstate->{loss}%"
						: "fping could not resolve host";

				# not multihomed? normal 'node down' handling
				if (!$thisstate->{has_sibling})
				{
					$nmisng->log->debug("Node $thisstate->{name} ($thisstate->{ip}) is NOT REACHABLE, $fpingmsg.");
					$whatevent = "node";
					$details = "Ping failed: fping reported 100% ping loss";
				}
				# multihomed and all dead, node down'
				elsif (List::Util::none { $_->{has_sibling} eq $thisstate->{has_sibling}
																	&& $_->{loss} != 100 } (values %state))
				{
					$nmisng->log->debug("Node $thisstate->{name} ($thisstate->{ip} and all siblings) is NOT REACHABLE, $fpingmsg.");
					$whatevent = "node";
					$details = "Ping failed: both primary and backup address are unreachable";
				}
				# multihomed but not all dead? different events for primary and backup address
				else
				{
					$nmisng->log->debug("Node $thisstate->{name} ($thisstate->{ip}) is NOT REACHABLE, $fpingmsg, but other siblings are up.");
					# node polling failover events are also raised by the snmp/wmi collect code, so clearing is race-y.
					$whatevent = ($thisstate->{is_primary}? "failover": "backup");
					$details = ($thisstate->{is_primary}?"Primary address":"Backup address")
							." $thisstate->{ip} is unreachable (but the other one is up).";
				}

				if ($whatevent)
				{
					# for unreachable/unresolvable nodes where we're caching the dns-ip association, we mark it as 'recheck dns'
					# for fixed-ip nodes this is not relevant
					undef $thisstate->{nextdns};
					if (!exists $thisstate->{loss})
					{
						$thisstate->{lastping} = $now;

						# what does the policy say about the interval?
						my $interval = ref($policies->{ $thisstate->{policy} }) eq "HASH"?
								$policies->{ $thisstate->{policy} }->{ping} : 60; # seconds

						$thisstate->{nextping} = $now + $interval;
					}

					# let's check the event existence before instantiating node and sys objects
					my $eventname = &NMISNG::Node::handle_down_eventnames->{$whatevent};
					my $event = $nmisng->events->event( node_uuid => $uuid, event => $eventname );
					if (!$event->exists)
					{
						$nmisng->log->debug("$uuid ($thisstate->{name}) is DOWN, raising event $eventname");

						my $node = $nmisng->node(uuid => $uuid);
						my $sys = NMISNG::Sys->new(nmisng => $nmisng);
						$sys->init(node => $node, snmp => 0);

						$node->handle_down( sys => $sys,
																type => $whatevent,
																up => 0,
																details => $details);
					}
				}
			}
			else # node somewhat pingable, not 100% loss
			{
				$nmisng->log->debug("$uuid ($thisstate->{name}) ($thisstate->{ip}) is pingable: returned min/avg/max = $thisstate->{min}/$thisstate->{avg}/$thisstate->{max}ms loss=$thisstate->{loss}%");
				my $details = "Ping ok: fping reported $thisstate->{loss}% loss";

				# what kind of event(s) do we have to cancel/close?
				# if not multihomed, or multihomed (and at least one of primary/backup is up,
				#  which is already certain when in this logic branch): node down
				# plus, if multihomed: 'backup host down' for the backup, or 'node polling failover' for the primary
				my @toclear = ("node");
				push @toclear, ($thisstate->{is_primary}? "failover" : "backup") if ($thisstate->{has_sibling});

				my ($sys, $node);
				for my $clearme (@toclear)
				{
					my $eventname = &NMISNG::Node::handle_down_eventnames->{$clearme};
					my $event = $nmisng->events->event( node_uuid => $uuid, event => $eventname );

					# any event-work required? then instantiate node (and sys) object and tell it to handle the up/down
					if ($event->exists && $event->active)
					{
						$logger->debug("$uuid ($thisstate->{name}) is now UP, closing event $eventname");
						if (!ref($node))
						{
							$node = $nmisng->node(uuid => $uuid);
							$sys = NMISNG::Sys->new(nmisng => $nmisng);
							$sys->init(node => $node, snmp => 0);
						}

						$node->handle_down( sys => $sys,
																type => $clearme,
																up => 1,
																details => $details);
					}
				}
			}
		}

		# we're done for this cycle, so how long should we sleep?
		# who's next? don't show gazillions of nodes unless debug level is really high
		if ($nmisng->log->detaillevel > 5)
		{
			my @soontonever = sort { $state{$a}->{nextping} <=> $state{$b}->{nextping} } (keys %state);
			$logger->debug("next ping ordering:\n\t"
										 . join("\n\t", map { sprintf("%s (%s) in %.2fs", $_,
																									$state{$_}->{name},
																									$state{$_}->{nextping}-$now) } (@soontonever)));
		}
		my $nextone = Statistics::Lite::min( map { $_->{nextping} } (values %state));

		# sleep if no cycle limit or not the final cycle yet
		my $naptime = ($nextone && $nextone - $now > 0)? int(0.5 + $nextone - $now) : 10;
		if ($naptime > 0 and (!defined($ttl) or $ttl > 1))
		{
			# cludge to get fping to sleep for less time.
			$naptime = 3 if $naptime == 5;
			$nmisng->log->debug("sleeping $naptime seconds");
			sleep($naptime);
		}


		my %goners = &NMISNG::Util::reaper;
		$nmisng->log->debug2("reaper cleared these pids and exitcodes: "
												 . Data::Dumper->new([\%goners])->Terse(1)->Indent(0)->Pair(": ")->Dump)
				if (%goners && $nmisng->log->is_level(2)); # don't bother with the dumper unless debug2 or higher

		--$ttl if (defined($ttl) && $ttl > 0);
	}
	return defined($$mustdie)? $$mustdie eq "INT"? EXIT_POLITE : EXIT_IMMEDIATELY
			: $max_cycles? EXIT_TTL : EXIT_ERROR;
}


__END__

signals for the nmisd processes
usr1 +verbose,
usr2 -verbose,
hup reopen log files
int polite shutdown (ie. when you're ready)
term standard termination, immediate
urg to wake up idle workers (get them out of sleep)


job structure:
type: operation
tag: marker for ops that need post-completion marshalling
args: hashref of stuff for the op
 collect, update, services expect uuid => single node uuid to work on
 collect, update, services allow force
 services: also want list of services
 schedule: fixme9 tbd - for telling the scheduler to do X at time Y; node,group,operation etc etc
 plugins: require phase = collect or update, and uuid => array of 'handled nodes' uuids
